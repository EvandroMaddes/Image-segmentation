# -*- coding: utf-8 -*-
"""challenge2.ipynb

Automatically generated by Colaboratory.

For the final part of the challenge we used the same notebook as the development phase but we integrated the new dataset. All the 4 teams have been trained with 4 different nets in order to discriminate better each team and improve the overall result. The nets that have been used are described in the report sent for the second challenge.

Download the dataset
"""

!wget 'https://competitions.codalab.org/my/datasets/download/df18097f-54f4-4faa-b09f-64bd4d7ac0e5'
!wget 'https://competitions.codalab.org/my/datasets/download/29a85805-2d8d-4701-a9ab-295180c89eb3'

"""Unzip the dataset"""

!unzip '/content/df18097f-54f4-4faa-b09f-64bd4d7ac0e5'
!unzip '/content/29a85805-2d8d-4701-a9ab-295180c89eb3'

"""CD to the dataset directory"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
#in order to hide a not dangerous warning
pd.options.mode.chained_assignment = None
from keras.preprocessing.image import ImageDataGenerator
from keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization
from keras.models import Sequential, Model
from keras.applications import VGG16
from keras.applications.vgg16 import preprocess_input #used to preprocess our data for the transfer learning approach
from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau
import json
import tensorflow as tf
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"
import os
import tensorflow as tf
import random
import shutil

"""## Selection of the decoder for our model.

Intall the external library in order to use segmentation models.
[Documentation for Segmentation Models](https://segmentation-models.readthedocs.io/en/latest/api.html)
"""

!pip install -U segmentation-models

# Commented out IPython magic to ensure Python compatibility.
# %env SM_FRAMEWORK=tf.keras

"""**Available Encoders**



    =============  ===== 
    Type           Names
    =============  =====
    VGG            'vgg16' 'vgg19'
    ResNet         'resnet18' 'resnet34' 'resnet50' 'resnet101' 'resnet152'
    SE-ResNet      'seresnet18' 'seresnet34' 'seresnet50' 'seresnet101' 'seresnet152'
    ResNeXt        'resnext50' 'resnext101'
    SE-ResNeXt     'seresnext50' 'seresnext101'
    SENet154       'senet154'
    DenseNet       'densenet121' 'densenet169' 'densenet201' 
    Inception      'inceptionv3' 'inceptionresnetv2'
    MobileNet      'mobilenet' 'mobilenetv2'
    EfficientNet   'efficientnetb0' 'efficientnetb1' 'efficientnetb2' 'efficientnetb3' 'efficientnetb4' 'efficientnetb5' efficientnetb6' efficientnetb7'
    =============  =====

"""

import segmentation_models as sm

BACKBONE = 'efficientnetb4' #select the encoder
preprocess_input = sm.get_preprocessing(BACKBONE) #set the preprocess function wrt the selected encoder

"""##Selection of the datasets used for training and testing.

"""

# Set the seed for random operations. 
# This let our experiments to be reproducible. 
SEED = 1212
tf.random.set_seed(SEED)
random.seed(SEED)
cwd = os.getcwd()
#image sizes
img_h = 512
img_w = 512

bs = 4 #batch size

classes = 3 #number of classes
#images are identified by the team and the seed
teams = ['Bipbip', 'Pead', 'Roseau', 'Weedelec']
seeds = ['Mais', 'Haricot']

#take the path of Development_Dataset and store it in dataset_dir
dataset_dir = os.path.join(cwd, 'Development_Dataset')
final_phase_dir = os.path.join(cwd, 'Test_Dev')

test_dir = os.path.join(cwd, 'Test_Final')


# modify here what team to train
teams_to_train = ['Bipbip'] #selection of the seed used during training
teams_to_test = ['Bipbip'] #selection of the seed used during testing

seeds_to_train = ['Mais', 'Haricot'] #selection of the seed used during training
seeds_to_test = ['Mais', 'Haricot'] #selection of the seeds used during testing

path_training =[]
path_testing =[]
#select path to the trining images
for team in teams_to_train:
    for seed in seeds_to_train:
        path_training.append(os.path.join(dataset_dir, 'Training', team, seed, 'Images'))
#select path to the test images
for team in teams:
    for seed in seeds:
        path_testing.append(os.path.join(test_dir, team, seed, 'Images'))

#move images from directory Test_Dev to correct directory Development_Dataset
for team in teams_to_train:
    for seed in seeds_to_train:
      org_img = os.path.join(final_phase_dir, team, seed, 'Images')
      org_msk = os.path.join(final_phase_dir, team, seed, 'Masks') 
      dest_img = os.path.join(dataset_dir, 'Training', team, seed, 'Images')
      dest_msk = os.path.join(dataset_dir, 'Training', team, seed, 'Masks') 
      for filename in os.listdir(org_img):
               shutil.move(os.path.join(org_img, filename), dest_img) #move images from final_set_dir to dataset_dir
      for filename in os.listdir(org_msk):
               shutil.move(os.path.join(org_msk, filename), dest_msk) #move masks from final_set_dir to dataset_dir

"""Two text files are created: one with training images paths and one with validation images path."""

create_txt = True

#destination of the text files
train_file = "train.txt"
valid_file = "valid.txt"
folders = path_training
#split training and validation set (80% - 20%)
split = 0.2

#write text files
if create_txt:
    with open(train_file, "w") as trainfile, open(valid_file, "w") as validfile:
      for folder in folders:
        for path,dirs,files in os.walk(folder):
            total_len = len(files)
            valid_len = round(len(files)*split)
            train_len = total_len - valid_len
            print("Found", total_len, "files, splitting into", valid_len, "(validation set) and", train_len, "(training set)")
            for i,fn in enumerate(sorted(files)):
                filename = os.path.splitext(fn)[0]
                if i >= valid_len:
                    trainfile.write("%s\n" % filename)
                else:
                    validfile.write("%s\n" % filename)

    print("\nSaving files to ", os.getcwd())
    trainfile.close()
    validfile.close()

# here we put all the original sizes of the test set into the map `images_sizes`
from PIL import Image
images_sizes = {}

for folder in path_testing:
    for path,dirs,files in os.walk(folder):
        for i,fn in enumerate(files):
            filename = os.path.splitext(fn)[0] + os.path.splitext(fn)[1] #take the filename
            curr_img = Image.open(os.path.join(path, filename)) 
            images_sizes[filename] = curr_img.size #take the image sizes

"""## Pre-processing of the data."""

from tensorflow.keras.preprocessing.image import ImageDataGenerator
#set to use or not data augmentation 
apply_data_augmentation = True

# Create training ImageDataGenerator object
# We need two different generators. One is for the images and the other one is for the corresponding masks
if apply_data_augmentation:
    img_data_gen = ImageDataGenerator(fill_mode='reflect',
                                      #rotation_range=10
                                      width_shift_range=10,
                                      height_shift_range=10,
                                      #zoom_range=0.3,
                                      #brightness_range= [0.5, 1],
                                      horizontal_flip=True,
                                      #vertical_flip=True
                                      )
    mask_data_gen = ImageDataGenerator(fill_mode='reflect',
                                       # rotation_range=10
                                      width_shift_range=10,
                                      height_shift_range=10,
                                      #zoom_range=0.3,
                                      #brightness_range= [0.5, 1],
                                      horizontal_flip=True,
                                      #vertical_flip=True
                                       )

"""## Create dataset objects."""

#define custom dataset, we start from the notebook seen at exercise session (multi-class segmentation)
class CustomDataset(tf.keras.utils.Sequence):

  def __init__(self, dataset_dir, which_subset, img_generator=None, mask_generator=None, 
               preprocessing_function=None, out_shape=[img_w, img_h]):
    if which_subset == 'training':
      subset_file = os.path.join(cwd, 'train.txt') #take training split of the images
    elif which_subset == 'validation':
      subset_file = os.path.join(cwd, 'valid.txt') #take validation spilt of the images
    
    with open(subset_file, 'r') as f:
      lines = f.readlines()
    
    subset_filenames = []
    for line in lines:
      subset_filenames.append(line.strip()) 

    self.which_subset = which_subset
    self.dataset_dir = dataset_dir
    self.subset_filenames = subset_filenames
    self.img_generator = img_generator
    self.mask_generator = mask_generator
    self.preprocessing_function = preprocessing_function
    self.out_shape = out_shape

  def __len__(self):
    return len(self.subset_filenames)

  def __getitem__(self, index):
    # Read Image
    curr_filename = self.subset_filenames[index]
    curr_team = curr_filename.split('_')[0] #take team name 
    curr_seed = curr_filename.split('_')[1].capitalize() #take seed type

    path = os.path.join(self.dataset_dir, curr_team, curr_seed)
    #for 'Roseau' dataset we have to handle the format of the images because they are png instead of jpg 
    if curr_team == 'Roseau':
        img = Image.open(os.path.join(path, 'Images', curr_filename + '.png'))
    else:
        img = Image.open(os.path.join(path, 'Images', curr_filename + '.jpg'))    
    mask = Image.open(os.path.join(path, 'Masks', curr_filename + '.png'))

    # Resize image and mask
    img = img.resize(self.out_shape)
    mask = mask.resize(self.out_shape, resample=Image.NEAREST)
    
    img_arr = np.array(img)
    mask_arr = np.array(mask)
    
    new_mask_arr = np.zeros(mask_arr.shape[:2], dtype=mask_arr.dtype)

    # Use RGB dictionary in 'RGBtoTarget.txt' to convert RGB to target
    new_mask_arr[np.where(np.all(mask_arr == [254, 124, 18], axis=-1))] = 0
    new_mask_arr[np.where(np.all(mask_arr == [255, 255, 255], axis=-1))] = 1
    new_mask_arr[np.where(np.all(mask_arr == [216, 67, 82], axis=-1))] = 2

    new_mask_arr = np.expand_dims(new_mask_arr, -1)
    
    if self.which_subset == 'training':
      if self.img_generator is not None and self.mask_generator is not None:
        # Perform data augmentation
        # We can get a random transformation from the ImageDataGenerator using get_random_transform
        # and we can apply it to the image using apply_transform
        img_t = self.img_generator.get_random_transform(img_arr.shape, seed=SEED)
        mask_t = self.mask_generator.get_random_transform(new_mask_arr.shape, seed=SEED)
        img_arr = self.img_generator.apply_transform(img_arr, img_t)
        # ImageDataGenerator use bilinear interpolation for augmenting the images.
        # Thus, when applied to the masks it will output 'interpolated classes', which
        # is an unwanted behaviour. As a trick, we can transform each class mask 
        # separately and then we can cast to integer values (as in the binary segmentation notebook).
        # Finally, we merge the augmented binary masks to obtain the final segmentation mask.
        out_mask = np.zeros_like(new_mask_arr)
        for c in np.unique(new_mask_arr):
          if c > 0:
            curr_class_arr = np.float32(new_mask_arr == c)
            curr_class_arr = self.mask_generator.apply_transform(curr_class_arr, mask_t)
            # from [0, 1] to {0, 1}
            curr_class_arr = np.uint8(curr_class_arr)
            # recover original class
            curr_class_arr = curr_class_arr * c 
            out_mask += curr_class_arr
    else:
      out_mask = new_mask_arr
    
    if self.preprocessing_function is not None:
        img_arr = self.preprocessing_function(img_arr)

    return img_arr, np.float32(out_mask)

#we define one dataset object for training set and one for validation set
#training
dataset = CustomDataset(os.path.join(dataset_dir, 'Training'), 'training', 
                        img_generator=img_data_gen, mask_generator=mask_data_gen,
                        preprocessing_function=preprocess_input)
#validation
dataset_valid = CustomDataset(os.path.join(dataset_dir, 'Training'), 'validation', 
                              preprocessing_function=preprocess_input)

#create
train_dataset = tf.data.Dataset.from_generator(lambda: dataset,
                                               output_types=(tf.float32, tf.float32),
                                               output_shapes=([img_h, img_w, 3], [img_h, img_w, 1]))

train_dataset = train_dataset.batch(bs)
#repeat
train_dataset = train_dataset.repeat()

#create
valid_dataset = tf.data.Dataset.from_generator(lambda: dataset_valid,
                                               output_types=(tf.float32, tf.float32),
                                               output_shapes=([img_h, img_w, 3], [img_h, img_w, 1]))
valid_dataset = valid_dataset.batch(bs)
#repeat
valid_dataset = valid_dataset.repeat()

#we define a dataset for the test set
# make basic intensity rescaling on test set. 
# If data augmentation is selected then preprocess_input function is applied to the input 
if apply_data_augmentation:
    test_data_gen = ImageDataGenerator(preprocessing_function=preprocess_input)
else:
    test_data_gen = ImageDataGenerator(rescale= 1./255)

# list of dataset choosen for which make predictions
list_datasets = []

for team in teams:
    for seed in seeds:
        curr_test_dir = os.path.join(test_dir, team, seed)
        test_gen = test_data_gen.flow_from_directory(directory=curr_test_dir,
                                                    batch_size=1,
                                                    class_mode=None,
                                                    classes=['Images'],
                                                    target_size=(img_h, img_w),
                                                    shuffle=False)
        #create
        test_dataset = tf.data.Dataset.from_generator(lambda: test_gen,
                                                    output_types=(tf.float32, tf.float32),
                                                    output_shapes=([img_h, img_w, 3], [img_h, img_w, 1]))
        list_datasets.append(test_gen)
        #repeat
        test_dataset.repeat()

"""##Functions used for tiling.

Functions to divide an images in more patches.
"""

#PARAMETERS OF THE PATCHES
patch_h=512 
patch_w=512
stride_h=0.5
stride_w=0.5

def return_padding(img, height, width):
    " Return padding given image and height, width of patch"
    h = 0 if img.shape[0]%height == 0 else height - img.shape[0]%height
    w = 0 if img.shape[1]%width == 0 else width - img.shape[1]%width
    pad_shape = tuple(np.zeros((len(img.shape),2),dtype=np.uint16))
    pad_shape = [tuple(x) for x in pad_shape]
    h_left  = h//2
    h_right = h - h_left
    w_left  = w//2
    w_right = w - w_left
    pad_shape[0] = (int(h_left),int(h_right))
    pad_shape[1] = (int(w_left),int(w_right))
    
    print("pad shape is {}".format(pad_shape))
    return pad_shape

def pad_image(img, height, width, channels=4, mode='constant'):
    """ 
        Pads img to make it fit for extracting patches of 
        shape height X width from it
        mode -> constant, reflect 
        constant -> pads ith 0's
        symmetric -> pads with reflection of image borders
    """
    print('input shape {}'.format(img.shape))
    pad_shape = return_padding(img, height, width)
    img = np.pad(img,pad_shape,mode='constant')
    print('output shape {}'.format(img.shape))
    return img

def create_patches(img, patch_height=patch_h, patch_width=patch_w, h_stride=stride_h, w_stride=stride_w):
    """ 
        Params: img -> Input image(numpy array)
                patch height -> height of patch to be cut
                patch width -> width of patch to be cut
                h_stride -> 1/overlap required among adjacent patch along height eg. 0.5 for twice overlap
                w_stride -> 1/overlap required among adjacent patch along width
        input -> image (height,width,channel), patch dimensions
    	output -> patches of desired dimensions (patch_height, patch_width, channel)
                patch parameters dictionary containing:
                                                          --original image height
                                                          --original image width
                                                          --stride along height
                                                          --stride along width
                                                          --patch height
                                                          --patch width
    """
    h_stride = int(max(1, patch_height * h_stride))
    w_stride = int(max(1, patch_width * w_stride))

    patch_param = {}
    patch_param['image_height'] = img.shape[0]
    patch_param['image_width'] = img.shape[1]
    patch_param['h_stride'] = h_stride
    patch_param['w_stride'] = w_stride
    patch_param['patch_height'] = patch_height
    patch_param['patch_width'] = patch_width

    h = 0
    w = 0

    img = pad_image(img, patch_height, patch_width)

    patches = []

    while h <= img.shape[0] - patch_height:
        w = 0
        while w <= img.shape[1] - patch_width:
            patches.append(img[h:h+patch_height, w:w+patch_width, :])
            w = w + w_stride
        h = h+h_stride

    return patches, patch_param

"""Functions to recombine patches of an image."""

def stitch_patch(patch_path, recon_img_path, image_dict, h_stride, w_stride, channel=3, type = 'png'):
    """
        Takes source folder containing patches of Images and reconstruct the original image by recombining
        the patches using naive overlapping assumption without any smoothing and saves them in destination
        NOTE: Patch files should be named like patch_i_j where i = Image number eg. 1,2,3,4 and j = Patch number
              eg. 1,2,3,4 etc. i.e. patch_i_j represent jth patch of ith image
        Params: patch_path -> source folder of patches
                recon_img_path -> destination folder of reconstructed image
                image_dict -> dictionary having image height, image width, patch height, patch width
                            with keys- 'image_height', 'image_width', 'patch_height', 'patch_width'
                h_stride -> 1/overlap taken among adjacent patch along height eg. 0.5 for twice overlap
                w_stride -> 1/overlap taken among adjacent patch along width
                channel  -> number of channel in patches
                type     -> type of patch 'png', 'jpg', 'tif'
    """
    
    if patch_path[-1] != '/':
        patch_path += '/'
  
    if not os.path.isdir(patch_path):
        raise Exception('patch directory does not exist')
    if not os.path.isdir(recon_img_path):
        print('creating destination folder')
        os.makedirs(destination)

    assert type in ['png', 'jpg', 'tif']
        
    patch_list = []
    i=1
    while True:
        patches = sorted(glob.glob(patch_path+'/patch_{}_*.tif'.format(i)), key=sortKeyFunc)
        if not patches:
            break
        patch_list.append(patches)
    for _ , _ , files in os.walk(patch_path):
        if not files:
            continue
        else:
            patch_height = int(image_dict['patch_height'])
            patch_width  = int(image_dict['patch_height'])
            img_id = files[0].split('/')[-1].split('_')[1]
            orig_img_height = int(image_dict['image_height'])
            orig_img_width  = int(image_dict['image_width'])
            h_stride = int(h_stride*patch_height)
            w_stride = int(w_stride*patch_width)

            img_dtype = np.uint16
            image     = np.zeros((orig_img_height, orig_img_width, channel), dtype = img_dtype)
            padding   = return_padding(image, patch_height, patch_width)
            image     = pad_zeros(image, patch_height, patch_width, channel)
            h = 0
            w = 0
            patches = []
            patch_id =0
            for name in files:
                try:
                    if type == 'tif':
                        io.use_plugin('tifffile')
                    patch = io.imread(name)
                    patches.append(patch)
                    if image.dtype != patch.dtype:
                        image = image.astype(patch.dtype, copy=False)                        
                except OSError as e:
                    print(e.errno)
                    print("Some of the patches are corrupted")

            while h <= image.shape[0]-patch_height:
                w = 0
                while w <= image.shape[1]-patch_width:
                    image[h:h+patch_height, w:w+patch_width, :] += patches[patch_id]
                    w = w + w_stride
                    patch_id+=1
                h = h+h_stride
            if(h_stride==w_stride):
                step = patch_height//h_stride
            else:
                print("Unequal strides are not yet suppported")

            mask_height = image.shape[0]//h_stride
            mask_width  = image.shape[1]//w_stride
            divisor_mask = make_divisor_mask(mask_height, mask_width, step)
            print("Divisor mask shape {}".format(divisor_mask.shape))

            h = 0
            w = 0
            mask_h = 0
            mask_w = 0
            print("Image shape {}".format(image.shape))
            while h <= image.shape[0] - h_stride:
                w = 0
                mask_w = 0
                while w <= image.shape[1] - w_stride:
                    image[h:h+h_stride, w:w+w_stride,:] /= divisor_mask[mask_h,mask_w]
                    w += w_stride
                    mask_w +=1
                h += h_stride
                mask_h +=1

            img = image[padding[0][0]:-padding[0][1], padding[1][0]:-padding[1][1],:]
            print("FinalImage shape{}".format(img.shape))
            assert img.shape == (orig_img_height, orig_img_width, channel)

            if not os.path.isdir(recon_img_path):
                os.mkdir(recon_img_path)

            io.imsave(recon_img_path + '/' + str(img_id) + '.' + type, img)
            print(str(recon_img_path + '/' + str(img_id) + '.' + type))


def sortKeyFunc(s):
  return int(os.path.basename(s).split('_')[2])

"""##Display of same images in order to understand the applaied data augmentation."""

# Commented out IPython magic to ensure Python compatibility.
# Let's test data generator
# -------------------------
import time
from matplotlib import cm
import matplotlib.pyplot as plt

# %matplotlib inline

# Assign a color to each class
evenly_spaced_interval =np.linspace(0, 1, 3)
colors = [cm.rainbow(x) for x in evenly_spaced_interval]

iterator = iter(valid_dataset)
iterator_train = iter(train_dataset)

fig, ax = plt.subplots(1, 3)

augmented_img, target = next(iterator)
train_img,_ = next(iterator_train)

augmented_img = augmented_img[0]   # First element
train_img = train_img[0]
augmented_img = augmented_img  # denormalize
train_img = train_img

target = np.array(target[0, ..., 0])   # First element (squeezing channel dimension)

print(np.unique(target))

target_img = np.zeros([target.shape[0], target.shape[1], 3])
target_img[np.where(target == 0)] = [0, 0, 0]

for i in range(1, 3):
  target_img[np.where(target == i)] = np.array(colors[i-1])[:3] * 255
#an image from the training set
ax[0].imshow(np.uint8(train_img))

ax[1].imshow(np.uint8(augmented_img))
ax[2].imshow(np.uint8(target_img))

plt.show()

"""##Selection of the decoder for our model."""

# define model
#we can select also others models: Unet, FPN, Linknet, PSPNet
model = sm.Unet(BACKBONE, encoder_weights='imagenet', classes=classes, activation='softmax')
model.summary()

"""## Optimization parameters: loss, learning rate, validation metrics."""

#loss function
loss = tf.keras.losses.SparseCategoricalCrossentropy() 
#loss = sm.losses.CategoricalCELoss()
#loss = sm.losses.CategoricalFocalLoss()
#loss = sm.losses.DiceLoss()
# ------------------

#learning rate
lr = 1e-5
# ------------------

#optimizer
optimizer = tf.keras.optimizers.Adam(learning_rate=lr)
# ------------------

#metrcs used to evaluate the model's performances
def meanIoU(y_true, y_pred):
    # get predicted class from softmax
    y_pred = tf.expand_dims(tf.argmax(y_pred, -1), -1)

    per_class_iou = []

    for i in range(1,classes): # exclude the background class 0
      # Get prediction and target related to only a single class (i)
      class_pred = tf.cast(tf.where(y_pred == i, 1, 0), tf.float32)
      class_true = tf.cast(tf.where(y_true == i, 1, 0), tf.float32)
      intersection = tf.reduce_sum(class_true * class_pred)
      union = tf.reduce_sum(class_true) + tf.reduce_sum(class_pred) - intersection
    
      iou = (intersection + 1e-7) / (union + 1e-7)
      per_class_iou.append(iou)

    return tf.reduce_mean(per_class_iou)
# ------------------

# Validation metrics
metrics = [meanIoU]
# ------------------

# Compile Model
model.compile(optimizer=optimizer, loss=loss, metrics=metrics)

"""## Define some *callbacks functions* that are going to be called at the end of each epoch.

We used *Early Stopping* in order to stop the training before the model overfits. Moreover we used *ReduceLROnPlateau* that helps the validation loss to decrease and slow down the training part in order to avoid overfitting. We then added another callback called *LearningRateScheduler* in order to tweak the learning rate after each epoch given a function `scheduler` .
"""

callbacks = []
# --------------
early_stop = True
attenuate_lr = True
schedule_lr = False

#early stopping is set with a patience of four epoch on the reduction (delta of 0.005) of the validation accuracy
if early_stop:
    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss',
                                          min_delta=0.001,
                                          patience = 2,
                                          verbose=1,
                                          mode='min',                                        
                                         )
    callbacks.append(es)
# ------------------

# Attenuate learning rate is set with a patience of three on the reduction of the validation accuracy.
#the new_lr is equal to the previous multiply by 0.3. this is repeated up to a minimum learninig rate of 10^(-7)
if attenuate_lr:
    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', 
                                                     mode='min', 
                                                     factor=0.3, 
                                                     patience=0, 
                                                     min_lr=1e-7, 
                                                     verbose=1, 
                                                     cooldown=0)
    callbacks.append(reduce_lr)
# ------------------

# This function keeps the initial learning rate for the first ten epochs and decreases it exponentially after that.  
def scheduler(epoch, lr):
    k = 0.1
    if epoch < 10:
        return lr
    else:
        return lr * tf.math.exp(-k)

if schedule_lr:
    cb = tf.keras.callbacks.LearningRateScheduler(schedule=scheduler, verbose=1)
    callbacks.append(cb)
# ------------------ 

#save checkpoint of the best model wrt the validation loss
callbacks.append(tf.keras.callbacks.ModelCheckpoint(filepath='model' + "".join(teams_to_train) + ".h5", monitor='val_loss', save_best_only=True))

"""##Training of the model."""

history = model.fit(x=train_dataset,
          epochs=50,
          steps_per_epoch=len(dataset)//bs,
          validation_data=valid_dataset,
          validation_steps=len(dataset_valid)//bs,
          callbacks=callbacks)

from keras.models import load_model
#we load one model for each dataset
model_bipbip = load_model('modelBipbip.h5', custom_objects={"meanIoU": meanIoU})
#model_roseau = load_model('modelRoseau.h5', custom_objects={"meanIoU": meanIoU})
#model_pead = load_model('modelPead.h5', custom_objects={"meanIoU": meanIoU})
#model_weedelec = load_model('modelWeedelec.h5', custom_objects={"meanIoU": meanIoU})

"""##Display some images with relative mask and prediction. """

# Commented out IPython magic to ensure Python compatibility.
import time
import matplotlib.pyplot as plt

from PIL import Image

# %matplotlib inline

iterator = iter(valid_dataset)

fig, ax = plt.subplots(1, 3, figsize=(8, 8))
fig.show()
image, target = next(iterator)

# take image, mask and predicted image
image = image[0]
target = target[0, ..., 0] 
out_sigmoid = model_bipbip.predict(x=tf.expand_dims(image, 0))
predicted_class = tf.argmax(out_sigmoid, -1)
predicted_class = predicted_class[0, ...]

# Assign colors (just for visualization)
target_img = np.zeros([target.shape[0], target.shape[1], 3])
prediction_img = np.zeros([target.shape[0], target.shape[1], 3])

target_img[np.where(target == 0)] = [0, 0, 0]
for i in range(1, 3):
  target_img[np.where(target == i)] = np.array(colors[i-1])[:3] * 255

prediction_img[np.where(predicted_class == 0)] = [0, 0, 0]
for i in range(1, 3):
  prediction_img[np.where(predicted_class == i)] = np.array(colors[i-1])[:3] * 255

#show images
ax[0].imshow(np.uint8(image)) #image
ax[1].imshow(np.uint8(target_img)) #mask
ax[2].imshow(np.uint8(prediction_img)) #predicted mask

fig.canvas.draw()
time.sleep(1)

"""## Prediction are created and reported on a zipped json file."""

import os
import json
import numpy as np
from PIL import Image


def rle_encode(img):
    '''
    img: numpy array, 1 - foreground, 0 - background
    Returns run length as string formatted
    '''
    pixels = img.flatten()
    pixels = np.concatenate([[0], pixels, [0]])
    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1
    runs[1::2] -= runs[::2]
    return ' '.join(str(x) for x in runs)

import ntpath
import sys
import codecs, json 
import pandas as pd

submission_dict = {}

printed = False
for curr_test_gen in list_datasets:
    
    curr_test_gen.reset()
    images = curr_test_gen.filenames

    # take team and seed names
    image_name = ntpath.basename(images[0])
    team_seed = image_name.split('_')
    curr_team = team_seed[0]
    curr_seed = team_seed[1].capitalize()

    # for each dataset we select the respective model to make predicitons
    if curr_team in teams_to_test and curr_seed in seeds_to_test:
        predictions = []
    
        if curr_team == 'Roseau':
            predictions = model_roseau.predict_generator(curr_test_gen, len(curr_test_gen), verbose=1)
        elif curr_team == 'Pead':
            predictions = model_pead.predict_generator(curr_test_gen, len(curr_test_gen), verbose=1)
        elif curr_team == 'Bipbip':
            predictions = model_bipbip.predict_generator(curr_test_gen, len(curr_test_gen), verbose=1)
        else:
            predictions = model_weedelec.predict_generator(curr_test_gen, len(curr_test_gen), verbose=1)
      
        i = 0
        # make predictions for all images in the current datatset
        for p in predictions:
            image_name = ntpath.basename(images[i])
            # roll back to the original size of the test image, using the map created above
            p = tf.image.resize(p, [images_sizes[image_name][1], images_sizes[image_name][0]], method='nearest')

            # remove extension
            image_name = image_name.split('.jpg')[0]
            image_name = image_name.split('.png')[0]
            i = i + 1

            img_name = image_name
            # -1 means do the max between the 3 classes (1536x2048x3), so the output will be 1536x2048x1
            mask_arr = tf.argmax(p, -1)
            mask_arr = np.array(mask_arr)

            if not printed:
                printed = True
            
            submission_dict[img_name] = {}
            submission_dict[img_name]['shape'] = mask_arr.shape
            submission_dict[img_name]['team'] = curr_team
            submission_dict[img_name]['crop'] = curr_seed
            submission_dict[img_name]['segmentation'] = {}

            # RLE encoding
            # crop
            rle_encoded_crop = rle_encode(mask_arr == 1)
            # weed
            rle_encoded_weed = rle_encode(mask_arr == 2)

            submission_dict[img_name]['segmentation']['crop'] = rle_encoded_crop
            submission_dict[img_name]['segmentation']['weed'] = rle_encoded_weed

just_tested_json = 'submission' + "".join(teams_to_test) + '.json'

with open(just_tested_json, 'w') as f:
    json.dump(submission_dict, f)

"""Merge together all the json files made."""

# choose to use predictions made by other models
merge_others = False

# put True if you want to merge other json files with the one just created
if merge_others:
    with open('submissionRoseauPead.json') as other_json, open(just_tested_json) as curr_test_json, open('submissionWeedelec.json') as other_json2:
        # ADD HERE ALL THE JSON YOU WANT TO MERGE
        json1 = json.load(other_json)
        json2 = json.load(curr_test_json)
        json3 = json.load(other_json2)
        # ...

        merged_json = {**json1, **json2, **json3} # ADD HERE ALL VARS CREATED

        with open('submission.json', 'w') as result_json:
            json.dump(merged_json, result_json)
        
        result_json.close()

    other_json.close()
    curr_test_json.close()

# create submission.json directly from the previous cell, without merge other jsons
else:
    with open('submission.json', 'w') as f:
        json.dump(submission_dict, f)

"""Create the zip of the json."""

from datetime import datetime
from zipfile import ZipFile
submission_name = "results.zip"
zipObj = ZipFile(submission_name, 'w')
zipObj.write('submission.json')
zipObj.close()

#code used to extract one team only(Weedelec) from submission.json

#with open('submission2mod.json') as other_json:
#    json1 = json.load(other_json)
#    new_dict = {}
#    for key, value in json1.items():
#        first = key.split('_')[0]
#        if first == 'Weedelec':
#            new_dict[key] = value
#    with open('submissionWeedelec.json', 'w') as result_json:
#            json.dump(new_dict, result_json)
